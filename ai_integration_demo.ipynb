{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a616d9f-36cf-497e-b10e-825a065ffa55",
   "metadata": {},
   "source": [
    "## AI Integration Demo\n",
    "The following notebook contains a comprehensive demonstration of the fundamental AI integration techniques I used during my time at SilverEdge Government Solutions from 2024-present. These techniques include:\n",
    "- Document text extraction via Apache Tika\n",
    "- Web Retrieval Augmented Generation (RAG) via Tavily\n",
    "- Token count estimation via Tiktoken\n",
    "- Creation and deployment of AI agents via LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730cd4f-d098-410e-acda-005f99b896e8",
   "metadata": {},
   "source": [
    "### 1) Text Extraction via Apache Tika\n",
    "\n",
    "<i>tika 3.1.0 (https://pypi.org/project/tika/)</i>\n",
    "\n",
    "The first step in creating a reliable AI agent is to implement a means of providing context to the Large Language Model (LLM). This is especially useful for cases where a user queries an agent about current events or something specific to their organization where information supporting a response is outside of the LLM's training data. First, we'll extract text from a local PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b77d8f-d0b6-4363-837b-dae44cb1faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika\n",
    "from tika import parser\n",
    "\n",
    "tika.initVM()\n",
    "\n",
    "# Extract text and metadata from a local PDF file\n",
    "parsed = parser.from_file('NLPPaper.pdf')\n",
    "\n",
    "pdf_metadata = parsed['metadata']\n",
    "pdf_content = parsed['content']\n",
    "\n",
    "print(pdf_metadata)\n",
    "print(pdf_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faee5f6-5255-4864-8b8b-7305f2d6fe1b",
   "metadata": {},
   "source": [
    "Now that the text has been successfully extracted, it's time to parse the Tika data into a LangChain-compatible object using the <i>LangChain Documents</i> module (https://reference.langchain.com/python/langchain_core/documents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db94e0-e7e8-4309-8ade-1ce9ff0e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "# Strip content of extra whitespaces\n",
    "cleaned_pdf_content = re.sub(r'\\s+', ' ', pdf_content)\n",
    "pdf_document = Document(page_content = cleaned_pdf_content, metadata = {'source': pdf_metadata.get('resourceName')})\n",
    "\n",
    "print(pdf_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edd5d2-9a53-4b71-addd-6e6edd838e02",
   "metadata": {},
   "source": [
    "### 2) Web RAG via Tavily\n",
    "\n",
    "<i> tavily 1.1.0 https://pypi.org/project/tavily/)</i>\n",
    "\n",
    "Next, we will generate more context for the LLM by performing a simple web search based on URL snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727513f3-1769-443a-808b-bc49a105ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(api_key = '')\n",
    "\n",
    "# User query to LLM\n",
    "query = 'What is NLP?'\n",
    "\n",
    "web_results = tavily.search(\n",
    "    query = query,\n",
    "    search_depth = 'basic',\n",
    "    max_results = 10,\n",
    "    include_raw_content = True\n",
    ")\n",
    "\n",
    "print(web_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86b37f-a1fa-43e8-873b-4d87047d4578",
   "metadata": {},
   "source": [
    "Now that we have the web results, let's parse them into a list of LangChain Documents like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea1bb9-e05b-4d7f-8d8d-a1ff428f3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "results = web_results.get('results', [])\n",
    "\n",
    "for result in results:\n",
    "    documents.append(Document(page_content = result.get('content'), metadata = {'source': result.get('title')}))\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eefd19-7313-4028-a111-6e9d16f5d121",
   "metadata": {},
   "source": [
    "Finally, we'll add the LangChain Document containing the extracted text from the PDF file. This will ensure the LLM has both internal and external context to support the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e4db1-38ad-48ab-b947-bca2257ebb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.append(pdf_document)\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea102d6-116f-4eb6-95fc-141d327774a6",
   "metadata": {},
   "source": [
    "### 3) Token Estimation via Tiktoken\n",
    "\n",
    "<i>tiktoken 0.12.0 (https://pypi.org/project/tiktoken/)</i>\n",
    "\n",
    "Now that we've finished the context preprocessing, we can execute a quick token estimation. Most LLMs have a maximum token parameter that constrains the input tokens (i.e., query and context) to a fixed number. Unfortunately, Tiktoken is primarily used for OpenAI models, so it won't be accurate for all models. However, this can still be quite useful in warning users when they may be approaching the model limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f5486-83e6-45bb-9681-18551036cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from typing import List\n",
    "\n",
    "def get_document_content(documents: List[Document]) -> str:\n",
    "    document_content = ''\n",
    "    \n",
    "    for document in documents:\n",
    "        document_content += document.page_content\n",
    "\n",
    "    cleaned_document_content = re.sub(r'\\s+', ' ', document_content)\n",
    "    return cleaned_document_content\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    # Byte Pair Encoding (BPE) tokenization technique (https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/)\n",
    "    encoding = tiktoken.get_encoding('o200k_base')\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Build context string for token estimate\n",
    "context_str = query + ' ' + get_document_content(documents)\n",
    "\n",
    "# Get token estimate\n",
    "print(f\"Token Estimate: {num_tokens_from_string(context_str)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ac4bf-7738-49cc-b4a4-cafbd01bea79",
   "metadata": {},
   "source": [
    "### 4) AI Agent Creation via LangChain\n",
    "\n",
    "<i>langchain 1.2.6 (https://pypi.org/project/langchain/) </i>\n",
    "\n",
    "LangChain is an open-source framework for building applications powered by LLMs. Users can integrate various commercial providers (e.g., Amazon, Azure, Oracle), access the latest LLMs, and connect them to external data through its comprehensive API (Python/JavaScript).\n",
    "\n",
    "In the following section, we're going to set up several LLM implementations and stream a response based on the query and context combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343f604-0c33-46ab-9c1c-d13c1b39fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum, auto\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_oci import ChatOCIGenAI\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# LLM Provider Options\n",
    "class LLMProvider(StrEnum):\n",
    "    AMAZON_BEDROCK = auto()\n",
    "    AZURE_OPENAI = auto()\n",
    "    ORACLE_CLOUD_INFRASTRUCTURE = auto()\n",
    "\n",
    "# LLM Provider Credentials\n",
    "\n",
    "# AWS Static Credentials\n",
    "aws_access_key = ''\n",
    "aws_secret_access_key = ''\n",
    "\n",
    "# Azure OpenAI Credentials\n",
    "azure_api_key = ''\n",
    "azure_endpoint = ''\n",
    "\n",
    "# Oracle Cloud Infrastructure Credentials\n",
    "oci_service_endpoint = ''\n",
    "oci_compartment_id = ''\n",
    "oci_user = ''\n",
    "oci_tenancy = ''\n",
    "oci_fingerprint = ''\n",
    "oci_region = ''\n",
    "oci_private_key = ''\n",
    "\n",
    "def get_llm_instance(provider: str):\n",
    "    if provider == LLMProvider.AMAZON_BEDROCK:\n",
    "        return ChatBedrock(\n",
    "            model_id = 'us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "            aws_access_key_id = aws_access_key,\n",
    "            aws_secret_access_key = aws_secret_access_key\n",
    "        )\n",
    "    if provider == LLMProvider.AZURE_OPENAI:\n",
    "        return AzureChatOpenAI(\n",
    "            azure_deployment = 'gpt-4o-mini',\n",
    "            api_version = '2025-01-01-preview',\n",
    "            azure_endpoint = azure_endpoint,\n",
    "            api_key = azure_api_key\n",
    "        )\n",
    "    if provider == LLMProvider.ORACLE_CLOUD_INFRASTRUCTURE:\n",
    "        oracle_config = {\n",
    "            'user': oci_user,\n",
    "            'tenancy': oci_tenancy,\n",
    "            'fingerprint': oci_fingerprint,\n",
    "            'region': oci_region,\n",
    "            'key_content': oci_private_key\n",
    "        }\n",
    "\n",
    "        client = GenerativeAiInferenceClient(\n",
    "            config = oracle_config,\n",
    "            service_endpoint = oci_service_endpoint\n",
    "        )\n",
    "\n",
    "        return ChatOCIGenAI(\n",
    "            client = client,\n",
    "            model_id = 'xai.grok-4',\n",
    "            compartment_id = oci_compartment_id\n",
    "        )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',\n",
    "     'You are a helpful assistant, skilled in finding facts and explaining the meaning behind them.\\n'\n",
    "     'You can draw on your own knowledge and use any documents provided.\\n'\n",
    "    ),\n",
    "    ('user', 'Uploaded Sources:\\n{context}\\n\\nQuery: {query}')\n",
    "])\n",
    "\n",
    "formatted_prompt = prompt.invoke({\n",
    "    'query': query,\n",
    "    'context': documents\n",
    "})\n",
    "\n",
    "llm = get_llm_instance(LLMProvider.ORACLE_CLOUD_INFRASTRUCTURE)\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d7bd-ad4d-48d3-8da9-222a3dd770de",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Through this walkthrough, we explored:\n",
    "\n",
    "- Unstructured data ingestion using Apache Tika to extract text from documents\n",
    "- Web-based Retrieval Augmented Generation (RAG) to enrich prompts with up-to-date external context\n",
    "- Token estimation to better understand performance and cost tradeoffs across different prompt sizes\n",
    "- Agent-based orchestration with LangChain, enabling flexible composition of tools and LLM interactions\n",
    "\n",
    "Taken together, these components reflect common patterns found in production AI systems: ingesting diverse data sources, grounding responses in external knowledge, managing operational constraints, and abstracting complexity through reusable agents.\n",
    "\n",
    "While the examples here are intentionally lightweight, the same patterns can be extended to support enterprise-scale use cases such as knowledge assistants, document analysis pipelines, decision-support tools, and AI-augmented workflows. The goal of this demo is not just to show what is possible with modern LLM tooling, but how these pieces fit together in a maintainable and scalable way.\n",
    "\n",
    "Future enhancements could include persistent vector stores, multi-agent collaboration, streaming responses, or integration with external APIs and databases. This notebook serves as a foundation for experimenting with and iterating on those ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9f9c5-1e3c-4360-8e29-4da54f295850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
